{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal, misc\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import scipy.io\n",
    "import emd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from pyentrp import entropy as ent\n",
    "import EntropyHub as EH\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'D:\\Downloads\\DTU\\semester 3\\Special course\\data\\Erladata/S/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_epochs(filename, s_data):\n",
    "    start = 0\n",
    "    end = 4000\n",
    "    step = int((end-start)/2)\n",
    "    count = 0\n",
    "    while end < 56001:\n",
    "        if 'dep' in filename.lower():\n",
    "            txt_path = data_path+'Depressed/'+filename[:-4]+str(count)+'.txt' #remove .txt from filename and append count and .txt extension\n",
    "            np.savetxt(txt_path, s_data[start:end].values)\n",
    "        elif 'healthy' in filename.lower():\n",
    "            txt_path = data_path+'Healthy/'+filename[:-4]+str(count)+'.txt'\n",
    "            np.savetxt(txt_path, s_data[start:end].values)\n",
    "        count += 1\n",
    "        start = start+step\n",
    "        end = end+step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy records which should be removed and not used\n",
    "noisy_recs = [\n",
    "    \"S1_pre_EC_DepActive_116s.txt\", \"S8_post_EC_DepActive_116s.txt\", \"S9_post_EC_Healthy_116s.txt\",\n",
    "    \"S9_pre_EC_Healthy_116s.txt\", \"S9_pre_EO_Healthy_116s.txt\", \"S15_pre_EC_Healthy_116s.txt\",\n",
    "    \"S15_pre_EO_Healthy_116s.txt\", \"S24_pre_EO_Healthy_116s.txt\", \"S29_post_EC_Healthy_116s.txt\",\n",
    "    \"S29_post_EO_Healthy_116s.txt\", \"S33_post_EC_DepSham_116s.txt\", \"S35_pre_EO_DepSham_116s.txt\",\n",
    "    \"S35_pre_EC_DepSham_116s.txt\", \"S35_post_EO_DepSham_116s.txt\", \"S35_post_EC_DepSham_116s.txt\",\n",
    "    \"S37_pre_EO_DepSham_116s.txt\", \"S37_pre_EC_DepSham_116s.txt\", \"S39_post_EC_Healthy_116s.txt\",\n",
    "    \"S39_post_EO_Healthy_116s.txt\", \"S39_pre_EC_Healthy_116s.txt\", \"S40_post_EC_DepSham_116s.txt\",\n",
    "    \"S40_post_EO_DepSham_116s.txt\", \"S40_pre_EC_DepSham_116s.txt\", \"S41_post_EC_DepSham_116s.txt\",\n",
    "    \"S41_post_EO_DepSham_116s.txt\", \"S41_pre_EC_DepSham_116s.txt\", \"S42_post_EC_Healthy_116s.txt\",\n",
    "    \"S42_pre_EO_Healthy_116s.txt\", \"S42_pre_EC_Healthy_116s.txt\", \"S44_post_EC_DepSham_116s.txt\",\n",
    "    \"S44_post_EO_DepSham_116s.txt\", \"S45_pre_EC_DepSham_116s.txt\", \"S45_pre_EO_DepSham_116s.txt\",\n",
    "    \"S45_post_EC_DepSham_116s.txt\", \"S45_post_EO_DepSham_116s.txt\", \"S46_post_EO_Healthy_116s.txt\",\n",
    "    \"S46_pre_EO_Healthy_116s.txt\", \"S46_pre_EC_Healthy_116s.txt\", \"S47_pre_EO_Healthy_116s.txt\",\n",
    "    \"S47_pre_EC_Healthy_116s.txt\", \"S52_pre_EC_DepActive_116s.txt\"\n",
    "]\n",
    "\n",
    "length = []\n",
    "for filename in os.listdir(data_path):\n",
    "    if (filename not in noisy_recs) and (\"pre\" in filename) and (filename.endswith(\".txt\")):\n",
    "        s_data = pd.read_csv(os.path.join(data_path, filename), sep=\"\\t\", index_col=False)\n",
    "        length.append(len(s_data))\n",
    "        divide_into_epochs(filename, s_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lengths\n",
       "60000      76\n",
       "56960       1\n",
       "57068       1\n",
       "57507       1\n",
       "58886       1\n",
       "59037       1\n",
       "59280       1\n",
       "59297       1\n",
       "59489       1\n",
       "59500       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_df = pd.DataFrame (length, columns = ['lengths'])\n",
    "length_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "#train and test data directory\n",
    "data_dir = \"../input/intel-image-classification/seg_train/seg_train/\"\n",
    "test_data_dir = \"../input/intel-image-classification/seg_test/seg_test\"\n",
    "\n",
    "\n",
    "#load the train and test data\n",
    "dataset = ImageFolder(data_dir,transform = transforms.Compose([\n",
    "    transforms.Resize((150,150)),transforms.ToTensor()\n",
    "]))\n",
    "test_dataset = ImageFolder(test_data_dir,transforms.Compose([\n",
    "    transforms.Resize((150,150)),transforms.ToTensor()\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42738bdaef2655e89af2c73ee93e50520a0ba3aa0d508c36067344d0f9aad4d2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('bci')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
